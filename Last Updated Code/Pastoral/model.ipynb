{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (144, 3)\n",
      "First few rows:\n",
      "   code                                    job description       label\n",
      "0  1398                   Managing a caseload of graduates  Response B\n",
      "1  1398  Providing support, advice, and information to ...  Response B\n",
      "2  1398     Tracking and reporting on graduate progression  Response B\n",
      "3  1398  Engaging with students regarding their well-be...  Response B\n",
      "4  1368  Providing leadership and ensuring compliance w...  Response B\n",
      "Columns: ['code', 'job description', 'label']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try loading with 'latin-1' encoding\n",
    "pastoral_df = pd.read_csv('pastoral.csv', encoding='latin-1')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", pastoral_df.shape)\n",
    "print(\"\\\n",
    "First few rows:\")\n",
    "print(pastoral_df.head())\n",
    "print(\"\\\n",
    "Columns:\", pastoral_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response A       0.67      0.36      0.47        11\n",
      "  Response B       0.70      0.89      0.78        18\n",
      "\n",
      "    accuracy                           0.69        29\n",
      "   macro avg       0.68      0.63      0.63        29\n",
      "weighted avg       0.68      0.69      0.66        29\n",
      "\n",
      "Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = pastoral_df['job description']\n",
    "y = pastoral_df['label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Transform text data to TF-IDF features\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Model Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "import joblib\n",
    "joblib.dump(rf_model, 'pastoral_care_model.joblib')\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.joblib')\n",
    "print(\"\\\n",
    "Model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Providing mentoring and support to students, including regular one-on-one sessions and workshops on personal development. Responsible for student welfare and maintaining records of support provided.\n",
      "Predicted Label: Response A\n",
      "Confidence: 65.00%\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "def predict_pastoral_care(job_description):\n",
    "    # Load the saved model and vectorizer\n",
    "    model = joblib.load('pastoral_care_model.joblib')\n",
    "    vectorizer = joblib.load('tfidf_vectorizer.joblib')\n",
    "    \n",
    "    # Transform the input text\n",
    "    job_description_tfidf = vectorizer.transform([job_description])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(job_description_tfidf)[0]\n",
    "    \n",
    "    # Get probability scores\n",
    "    probabilities = model.predict_proba(job_description_tfidf)[0]\n",
    "    confidence = max(probabilities) * 100\n",
    "    \n",
    "    return prediction, confidence\n",
    "\n",
    "# Test the function with a sample job description\n",
    "test_description = \"Providing mentoring and support to students, including regular one-on-one sessions and workshops on personal development. Responsible for student welfare and maintaining records of support provided.\"\n",
    "\n",
    "prediction, confidence = predict_pastoral_care(test_description)\n",
    "print(\"Job Description:\", test_description)\n",
    "print(\"\\\n",
    "Predicted Label:\", prediction)\n",
    "print(\"Confidence: {:.2f}%\".format(confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.30.2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (4.30.2)\n",
      "Requirement already satisfied: torch==2.0.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers==4.30.2) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from torch==2.0.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from torch==2.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from torch==2.0.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from torch==2.0.1) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests->transformers==4.30.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests->transformers==4.30.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests->transformers==4.30.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests->transformers==4.30.2) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Let's try a different approach using BERT with transformers\n",
    "%pip install transformers==4.30.2 torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 10477: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the CSV file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m pastoral_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpastoral.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the dataframe to understand its structure\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(pastoral_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd5 in position 10477: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "pastoral_df = pd.read_csv('pastoral.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "print(pastoral_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug==1.1.10 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (1.1.10)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from nlpaug==1.1.10) (1.26.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from nlpaug==1.1.10) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from nlpaug==1.1.10) (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from pandas>=1.2.0->nlpaug==1.1.10) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from pandas>=1.2.0->nlpaug==1.1.10) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from pandas>=1.2.0->nlpaug==1.1.10) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests>=2.22.0->nlpaug==1.1.10) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests>=2.22.0->nlpaug==1.1.10) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests>=2.22.0->nlpaug==1.1.10) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests>=2.22.0->nlpaug==1.1.10) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.5 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug==1.1.10) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/leandrenash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Augment the dataset\u001b[39;00m\n\u001b[1;32m     25\u001b[0m augmented_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Original data\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     augmented_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob description\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjob description\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     32\u001b[0m     })\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Augmented data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "%pip install nlpaug==1.1.10\n",
    "\n",
    "# Re-import the necessary modules after installation\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize augmenter\n",
    "aug = naw.SynonymAug(aug_p=0.3)\n",
    "\n",
    "# Function to augment text while preserving key terms\n",
    "def augment_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    augmented_sentences = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            aug_text = aug.augment(sentence)[0]\n",
    "            augmented_sentences.append(aug_text)\n",
    "        except:\n",
    "            augmented_sentences.append(sentence)\n",
    "    return ' '.join(augmented_sentences)\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_data = []\n",
    "for _, row in df.iterrows():\n",
    "    # Original data\n",
    "    augmented_data.append({\n",
    "        'code': row['code'],\n",
    "        'job description': row['job description'],\n",
    "        'label': row['label']\n",
    "    })\n",
    "    \n",
    "    # Augmented data\n",
    "    for _ in range(2):  # Create 2 augmented versions\n",
    "        augmented_data.append({\n",
    "            'code': row['code'],\n",
    "            'job description': augment_text(row['job description']),\n",
    "            'label': row['label']\n",
    "        })\n",
    "\n",
    "# Create new dataframe with augmented data\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "print(\"Original dataset size:\", len(df))\n",
    "print(\"Augmented dataset size:\", len(augmented_df))\n",
    "print(\"\\\n",
    "Sample of augmented data:\")\n",
    "print(augmented_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape: (144, 3)\n",
      "Columns: ['code', 'job description', 'label']\n",
      "First few rows:\n",
      "   code                                    job description       label\n",
      "0  1398                   Managing a caseload of graduates  Response B\n",
      "1  1398  Providing support, advice, and information to ...  Response B\n",
      "2  1398     Tracking and reporting on graduate progression  Response B\n",
      "3  1398  Engaging with students regarding their well-be...  Response B\n",
      "4  1368  Providing leadership and ensuring compliance w...  Response B\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file and verify its structure\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('pastoral.csv', encoding='latin-1')\n",
    "print(\"DataFrame Shape:\", df.shape)\n",
    "print(\"\\\n",
    "Columns:\", df.columns.tolist())\n",
    "print(\"\\\n",
    "First few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 115\n",
      "Testing samples: 29\n",
      "Label classes: ['Response A' 'Response B']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 15/15 [00:50<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average loss: 2.1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 15/15 [00:49<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average loss: 2.0815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 15/15 [00:48<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average loss: 2.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 15/15 [00:48<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Average loss: 1.8808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 15/15 [00:48<00:00,  3.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Average loss: 1.6962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 15/15 [00:48<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Average loss: 1.6013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 15/15 [00:52<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Average loss: 1.1764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 15/15 [00:49<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Average loss: 0.9074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 15/15 [00:53<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Average loss: 0.7095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 15/15 [00:46<00:00,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Average loss: 0.3736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create separate labels for each question (40, 41, 42)\n",
    "df['q40_label'] = df['label']\n",
    "df['q41_label'] = df['label']\n",
    "df['q42_label'] = df['label']\n",
    "\n",
    "# Initialize label encoder\n",
    "le = LabelEncoder()\n",
    "df['q40_label_encoded'] = le.fit_transform(df['q40_label'])\n",
    "df['q41_label_encoded'] = le.transform(df['q41_label'])\n",
    "df['q42_label_encoded'] = le.transform(df['q42_label'])\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Custom dataset class\n",
    "class JobDescriptionDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = tokenizer(text, return_tensors='pt', padding='max_length', \n",
    "                           truncation=True, max_length=512)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = JobDescriptionDataset(\n",
    "    train_df['job description'].values,\n",
    "    train_df[['q40_label_encoded', 'q41_label_encoded', 'q42_label_encoded']].values\n",
    ")\n",
    "\n",
    "test_dataset = JobDescriptionDataset(\n",
    "    test_df['job description'].values,\n",
    "    test_df[['q40_label_encoded', 'q41_label_encoded', 'q42_label_encoded']].values\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Testing samples:\", len(test_dataset))\n",
    "print(\"\\\n",
    "Label classes:\", le.classes_)\n",
    "\n",
    "# Define the model architecture\n",
    "class JobClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)  # Increased dropout for better regularization\n",
    "        self.classifier = nn.Linear(768, n_classes * 3)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[0][:, 0]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits.view(-1, 3, 2)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = JobClassifier(n_classes=2).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  # Lower learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "print(f\"Training on {device}\")\n",
    "\n",
    "# Training loop with early stopping\n",
    "n_epochs = 10\n",
    "best_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{n_epochs}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(3):\n",
    "            loss += criterion(outputs[:, i], labels[:, i])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1} - Average loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        \n",
    "    if no_improve >= patience:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy on Test Set: 74.71%\n",
      "Predictions for Sample Job Description:\n",
      "Question 40: Response A (Confidence: 77.79%)\n",
      "Question 41: Response A (Confidence: 78.85%)\n",
      "Question 42: Response A (Confidence: 76.85%)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            for i in range(3):  # For each question\n",
    "                _, preds = torch.max(outputs[:, i], dim=1)\n",
    "                correct_predictions += torch.sum(preds == labels[:, i])\n",
    "                total_predictions += len(labels)\n",
    "    \n",
    "    accuracy = correct_predictions.double() / total_predictions\n",
    "    return accuracy.item()\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluate_model(model, test_loader)\n",
    "print(f'Model Accuracy on Test Set: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Function to predict labels for a new job description\n",
    "def predict_labels(job_description):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        job_description,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        \n",
    "        for i in range(3):  # For each question\n",
    "            probs = torch.softmax(outputs[:, i], dim=1)\n",
    "            confidence, preds = torch.max(probs, dim=1)\n",
    "            predictions.append(le.inverse_transform(preds.cpu().numpy())[0])\n",
    "            probabilities.append(confidence.item() * 100)\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "# Test with a sample job description\n",
    "sample_description = \"\"\"\n",
    "Leading and managing a team of pastoral care staff, providing guidance and support to students.\n",
    "Developing and implementing pastoral care programs and initiatives.\n",
    "Conducting one-on-one counseling sessions with students facing personal or academic challenges.\n",
    "Coordinating with teachers and parents to ensure comprehensive support for students' wellbeing.\n",
    "\"\"\"\n",
    "\n",
    "predictions, confidences = predict_labels(sample_description)\n",
    "\n",
    "print(\"\\\n",
    "Predictions for Sample Job Description:\")\n",
    "print(\"Question 40:\", predictions[0], f\"(Confidence: {confidences[0]:.2f}%)\")\n",
    "print(\"Question 41:\", predictions[1], f\"(Confidence: {confidences[1]:.2f}%)\")\n",
    "print(\"Question 42:\", predictions[2], f\"(Confidence: {confidences[2]:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Sample Job Description:\n",
      "Question 40: Response B\n",
      "Question 41: Response B\n",
      "Question 42: Response B\n"
     ]
    }
   ],
   "source": [
    "# Function to predict labels for a new job description without percentages\n",
    "def predict_labels_simple(job_description):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        job_description,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(3):  # For each question\n",
    "            _, preds = torch.max(outputs[:, i], dim=1)\n",
    "            predictions.append(le.inverse_transform(preds.cpu().numpy())[0])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test with a sample job description\n",
    "sample_description = \"\"\"\n",
    "The Web Services Manager, part of the Marketing, Communications, and Recruitment Department (Grade 8a), works 35 hours per week, typically from 9:00 am to 5:00 pm, Monday to Friday, with a one-hour lunch break. They are responsible to the Head of Marketing and Recruitment and oversee the Web Developer. The role involves managing the technical infrastructure and framework for Web Services, including the externally facing website and the Web Content Management System (WCMS), to ensure a seamless user experience that aligns with institutional objectives. Key duties include managing the institutional CMS, developing responsive style templates, integrating the CMS with corporate systems, and overseeing the deployment of bespoke code like HTML, CSS, and JavaScript for high-performing, SEO-optimized, user-friendly, and secure websites. Additionally, the manager leads the development of the digital governance framework, including maintaining the University's Pattern Library, and oversees the development of web tools that enhance user experience. They act as the main point of contact for web-related technologies, plan future web services developments, generate reports for leadership, and ensure the effective implementation of the University’s Web Strategy. Other responsibilities include user testing, using analytics to enhance functionality, managing the web budget, documenting code and processes, collaborating with Marketing, Recruitment, and Communications team members, liaising with external suppliers, and ensuring the upkeep of associated systems like the Search Tool and Asset Management Library. The Web Services Manager also supports digital transformation by promoting new technologies and trends, manages web support duties with Digital Team members, ensures secure system hosting in collaboration with IT services, and trains staff on CMS usage. Leadership responsibilities include managing the Web Developer, fostering teamwork, and driving transformational digital change. The role also involves compliance with GDPR, health and safety regulations, and the University's Equality, Diversity, and Inclusion Policy, with flexibility to undertake additional duties as required. The job description is subject to change to accommodate institutional developments, last updated in July 2021\n",
    "\"\"\"\n",
    "\n",
    "predictions = predict_labels_simple(sample_description)\n",
    "\n",
    "print(\"Predictions for Sample Job Description:\")\n",
    "print(\"Question 40:\", predictions[0])\n",
    "print(\"Question 41:\", predictions[1])\n",
    "print(\"Question 42:\", predictions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Sample Job Description:\n",
      "Question 40: Response A\n",
      "Question 41: Response A\n",
      "Question 42: Response A\n"
     ]
    }
   ],
   "source": [
    "# Function to predict labels for a new job description without percentages\n",
    "def predict_labels_simple(job_description):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        job_description,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(3):  # For each question\n",
    "            _, preds = torch.max(outputs[:, i], dim=1)\n",
    "            predictions.append(le.inverse_transform(preds.cpu().numpy())[0])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test with a sample job description\n",
    "sample_description = \"\"\"\n",
    "The purpose of the role is to contribute to the delivery of taught modules and extra-curricular sessions for undergraduate, postgraduate, apprenticeship, and work-based programmes by providing lived experiences as a guest speaker, aiding curriculum development, and advising on opportunities for the University in West Yorkshire. Reporting to the Director of the Centre for Apprenticeships, Work-Based Learning, and Skills, the key responsibilities include developing and delivering workshops on important topics relevant to the West Yorkshire region, such as race, ethnicity, religion, and economic and social history. Additionally, the role involves contributing to extra-curricular activities and Equality & Diversity events hosted by the University or its partners, supporting recruitment activities, and maintaining links with external organizations to support these contributions. The individual will also advise on expanding the Ambassador network, identifying gaps in provision, and enhancing the student experience through further expertise. Furthermore, they will set up additional activities to raise the University's external profile. General duties include ensuring data use complies with regulations, particularly GDPR, adhering to health, safety, and wellbeing policies, promoting safeguarding and protection of others, applying the University's Equality, Diversity, and Inclusion Policy, and performing other duties as directed by their line manager. This job description is subject to variation by the Vice-Chancellor to reflect institutional developments\n",
    "\"\"\"\n",
    "\n",
    "predictions = predict_labels_simple(sample_description)\n",
    "\n",
    "print(\"Predictions for Sample Job Description:\")\n",
    "print(\"Question 40:\", predictions[0])\n",
    "print(\"Question 41:\", predictions[1])\n",
    "print(\"Question 42:\", predictions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Sample Job Description:\n",
      "Question 40: Response A\n",
      "Question 41: Response A\n",
      "Question 42: Response A\n"
     ]
    }
   ],
   "source": [
    "# Function to predict labels for a new job description without percentages\n",
    "def predict_labels_simple(job_description):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        job_description,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(3):  # For each question\n",
    "            _, preds = torch.max(outputs[:, i], dim=1)\n",
    "            predictions.append(le.inverse_transform(preds.cpu().numpy())[0])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test with a sample job description\n",
    "sample_description = \"\"\"\n",
    "The purpose of the role is to create, implement, and manage sustainable solutions for the University's needs across all campus locations by evaluating the environmental impact of day-to-day activities and developments. The Sustainability Manager is responsible for supporting the management, monitoring, and reporting of the institution’s efforts to reduce its carbon footprint across scopes 1, 2, and 3 emissions, managing waste, recycling, and construction activities, and sourcing sustainable energy. The role also includes implementing new green technologies to achieve a net zero carbon status and minimizing the environmental impact of daily operations. Reporting to the Estates Project Development Manager, key responsibilities include sourcing solutions to reduce the University's environmental impact in areas such as materials, waste, energy, and water management, and ensuring compliance with environmental legislation. The role involves conducting ecology surveys, performing environmental impact assessments, and completing mandatory assessments like BREEAM. The manager will collect and analyze data to produce reports on the University's current status and recommend strategies for improvement, including cost-benefit analyses and timelines for implementation. They will advise on compliance with environmental law and suggest cost-effective technologies to lower energy emissions and waste. The role requires staying updated on research and legislation through seminars, publications, and social media. The Sustainability Manager will manage sustainability projects, prepare project bid documents, and ensure compliance with University policies. They will also oversee sustainability initiatives, ensuring the effective use of resources, compliance with legal requirements for waste management, and providing sustainability guidance on construction projects. The role includes maintaining the University’s sustainability and environmental policies and contributing to the management of carbon footprint reduction. The manager will work closely with the Head of Estates and Facilities and the Head of Commercial Enterprises to ensure sustainable and environmental responsibilities are managed effectively. General duties include ensuring compliance with GDPR, adhering to health, safety, and wellbeing policies, promoting safeguarding, applying the University’s Equality, Diversity, and Inclusion Policy, and performing other duties as directed by the line manager. This job description is subject to variation by the Vice-Chancellor to reflect institutional developments.\n",
    "\"\"\"\n",
    "\n",
    "predictions = predict_labels_simple(sample_description)\n",
    "\n",
    "print(\"Predictions for Sample Job Description:\")\n",
    "print(\"Question 40:\", predictions[0])\n",
    "print(\"Question 41:\", predictions[1])\n",
    "print(\"Question 42:\", predictions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for Sample Job Description:\n",
      "Question 40: Response B\n",
      "Question 41: Response B\n",
      "Question 42: Response B\n"
     ]
    }
   ],
   "source": [
    "# Function to predict labels for a new job description without percentages\n",
    "def predict_labels_simple(job_description):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        job_description,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(3):  # For each question\n",
    "            _, preds = torch.max(outputs[:, i], dim=1)\n",
    "            predictions.append(le.inverse_transform(preds.cpu().numpy())[0])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Test with a sample job description\n",
    "sample_description = \"\"\"\n",
    "The Web Services Manager, part of the Marketing, Communications, and Recruitment Department (Grade 8a), works 35 hours per week, typically from 9:00 am to 5:00 pm, Monday to Friday, with a one-hour lunch break. They are responsible to the Head of Marketing and Recruitment and oversee the Web Developer. The role involves managing the technical infrastructure and framework for Web Services, including the externally facing website and the Web Content Management System (WCMS), to ensure a seamless user experience that aligns with institutional objectives. Key duties include managing the institutional CMS, developing responsive style templates, integrating the CMS with corporate systems, and overseeing the deployment of bespoke code like HTML, CSS, and JavaScript for high-performing, SEO-optimized, user-friendly, and secure websites. Additionally, the manager leads the development of the digital governance framework, including maintaining the University's Pattern Library, and oversees the development of web tools that enhance user experience. They act as the main point of contact for web-related technologies, plan future web services developments, generate reports for leadership, and ensure the effective implementation of the University’s Web Strategy. Other responsibilities include user testing, using analytics to enhance functionality, managing the web budget, documenting code and processes, collaborating with Marketing, Recruitment, and Communications team members, liaising with external suppliers, and ensuring the upkeep of associated systems like the Search Tool and Asset Management Library. The Web Services Manager also supports digital transformation by promoting new technologies and trends, manages web support duties with Digital Team members, ensures secure system hosting in collaboration with IT services, and trains staff on CMS usage. Leadership responsibilities include managing the Web Developer, fostering teamwork, and driving transformational digital change. The role also involves compliance with GDPR, health and safety regulations, and the University's Equality, Diversity, and Inclusion Policy, with flexibility to undertake additional duties as required. The job description is subject to change to accommodate institutional developments, last updated in July 2021\n",
    "\"\"\"\n",
    "\n",
    "predictions = predict_labels_simple(sample_description)\n",
    "\n",
    "print(\"Predictions for Sample Job Description:\")\n",
    "print(\"Question 40:\", predictions[0])\n",
    "print(\"Question 41:\", predictions[1])\n",
    "print(\"Question 42:\", predictions[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“virtual”env”",
   "language": "python",
   "name": "myvirtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
