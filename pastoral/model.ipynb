{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation completed. BERT tokenizer and DataLoaders are ready.\n",
      "Number of training samples: 115\n",
      "Number of validation samples: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('pastoral.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply text cleaning to 'job description' column\n",
    "df['cleaned_description'] = df['job description'].apply(clean_text)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['encoded_label'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['cleaned_description'], df['encoded_label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels.tolist())\n",
    ")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(val_encodings['input_ids']),\n",
    "    torch.tensor(val_encodings['attention_mask']),\n",
    "    torch.tensor(val_labels.tolist())\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\"Data preparation completed. BERT tokenizer and DataLoaders are ready.\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.2\n",
      "Tokenizer initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check the version of the transformers library\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "# Reinitialize the BERT tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    print(\"Tokenizer initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error initializing tokenizer:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (4.44.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.0-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.44.2\n",
      "    Uninstalling transformers-4.44.2:\n",
      "      Successfully uninstalled transformers-4.44.2\n",
      "Successfully installed tokenizers-0.20.0 transformers-4.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation completed. BERT tokenizer and DataLoaders are ready.\n",
      "Number of training samples: 115\n",
      "Number of validation samples: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for training on cpu\n",
      "Epoch 1/4\n",
      "Training loss: 0.6983\n",
      "Validation Accuracy: 0.7241\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response A       0.71      0.45      0.56        11\n",
      "  Response B       0.73      0.89      0.80        18\n",
      "\n",
      "    accuracy                           0.72        29\n",
      "   macro avg       0.72      0.67      0.68        29\n",
      "weighted avg       0.72      0.72      0.71        29\n",
      "\n",
      "Epoch 2/4\n",
      "Training loss: 0.6801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6207\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response A       0.00      0.00      0.00        11\n",
      "  Response B       0.62      1.00      0.77        18\n",
      "\n",
      "    accuracy                           0.62        29\n",
      "   macro avg       0.31      0.50      0.38        29\n",
      "weighted avg       0.39      0.62      0.48        29\n",
      "\n",
      "Epoch 3/4\n",
      "Training loss: 0.6258\n",
      "Validation Accuracy: 0.7931\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response A       0.73      0.73      0.73        11\n",
      "  Response B       0.83      0.83      0.83        18\n",
      "\n",
      "    accuracy                           0.79        29\n",
      "   macro avg       0.78      0.78      0.78        29\n",
      "weighted avg       0.79      0.79      0.79        29\n",
      "\n",
      "Epoch 4/4\n",
      "Training loss: 0.5653\n",
      "Validation Accuracy: 0.8276\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response A       0.75      0.82      0.78        11\n",
      "  Response B       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.83        29\n",
      "   macro avg       0.82      0.83      0.82        29\n",
      "weighted avg       0.83      0.83      0.83        29\n",
      "\n",
      "Training completed.\n",
      "Sample Job Description: Providing academic advice and support to students throughout their studies\n",
      "Predicted Label: Response A\n",
      "Model is ready for predictions. You can now input job descriptions to get labeled responses.\n"
     ]
    }
   ],
   "source": [
    "# Update transformers library\n",
    "%pip install --upgrade transformers\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('pastoral.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply text cleaning to 'job description' column\n",
    "df['cleaned_description'] = df['job description'].apply(clean_text)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['encoded_label'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['cleaned_description'], df['encoded_label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels.tolist())\n",
    ")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(val_encodings['input_ids']),\n",
    "    torch.tensor(val_encodings['attention_mask']),\n",
    "    torch.tensor(val_labels.tolist())\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\"Data preparation completed. BERT tokenizer and DataLoaders are ready.\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(le.classes_),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded and ready for training on {device}\")\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels': batch[2]}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(inputs['labels'].cpu().numpy())\n",
    "    return accuracy_score(true_labels, predictions), classification_report(true_labels, predictions, target_names=le.classes_)\n",
    "\n",
    "# Training loop\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train(model, train_dataloader, optimizer, device)\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    val_accuracy, val_report = evaluate(model, val_dataloader, device)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(val_report)\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Function to predict label for a new job description\n",
    "def predict_label(job_description):\n",
    "    cleaned = clean_text(job_description)\n",
    "    inputs = tokenizer(cleaned, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "    return le.inverse_transform([prediction])[0]\n",
    "\n",
    "# Test the model with a sample job description\n",
    "sample_job = \"Providing academic advice and support to students throughout their studies\"\n",
    "predicted_label = predict_label(sample_job)\n",
    "print(f\"\\\n",
    "Sample Job Description: {sample_job}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "print(\"\\\n",
    "Model is ready for predictions. You can now input job descriptions to get labeled responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search for RandomForest...\n",
      "Best parameters for RandomForest: {'max_depth': 20, 'n_estimators': 200}\n",
      "Best cross-validated accuracy for RandomForest: 0.63\n",
      "Performing grid search for GradientBoosting...\n",
      "Best parameters for GradientBoosting: {'learning_rate': 0.01, 'n_estimators': 100}\n",
      "Best cross-validated accuracy for GradientBoosting: 0.60\n",
      "Performing grid search for SVM...\n",
      "Best parameters for SVM: {'C': 10, 'kernel': 'linear'}\n",
      "Best cross-validated accuracy for SVM: 0.66\n",
      "Test Accuracy for RandomForest: 0.69\n",
      "Classification Report for RandomForest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response A       0.60      0.55      0.57        11\n",
      "  Response B       0.74      0.78      0.76        18\n",
      "\n",
      "    accuracy                           0.69        29\n",
      "   macro avg       0.67      0.66      0.66        29\n",
      "weighted avg       0.68      0.69      0.69        29\n",
      "\n",
      "Test Accuracy for GradientBoosting: 0.72\n",
      "Classification Report for GradientBoosting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response A       0.80      0.36      0.50        11\n",
      "  Response B       0.71      0.94      0.81        18\n",
      "\n",
      "    accuracy                           0.72        29\n",
      "   macro avg       0.75      0.65      0.65        29\n",
      "weighted avg       0.74      0.72      0.69        29\n",
      "\n",
      "Test Accuracy for SVM: 0.66\n",
      "Classification Report for SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response A       0.56      0.45      0.50        11\n",
      "  Response B       0.70      0.78      0.74        18\n",
      "\n",
      "    accuracy                           0.66        29\n",
      "   macro avg       0.63      0.62      0.62        29\n",
      "weighted avg       0.65      0.66      0.65        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "# Define models and parameters for grid search\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'RandomForest': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'GradientBoosting': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1]},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "}\n",
    "\n",
    "# Perform grid search for each model\n",
    "best_models = {}\n",
    "for model_name in models:\n",
    "    print(f\"\\\n",
    "Performing grid search for {model_name}...\")\n",
    "    grid_search = GridSearchCV(models[model_name], params[model_name], cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validated accuracy for {model_name}: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Evaluate the best models on the test set\n",
    "for model_name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\\n",
    "Test Accuracy for {model_name}: {accuracy:.2f}\")\n",
    "    print(f\"Classification Report for {model_name}:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 1.3.2\n",
      "Successfully imported SMOTE\n",
      "imbalanced-learn             0.12.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully imported SMOTE after installation\n",
      "Required scikit-learn version for imbalanced-learn: 1.0.2 or later\n",
      "Current scikit-learn version: 1.3.2\n",
      "Successfully imported SMOTE after all updates\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "\n",
    "# Try importing SMOTE from imbalanced-learn\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    print(\"Successfully imported SMOTE\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing SMOTE: {e}\")\n",
    "\n",
    "# Check if imbalanced-learn is installed\n",
    "%pip list | grep imbalanced-learn\n",
    "\n",
    "# If not installed, install it\n",
    "%pip install imbalanced-learn\n",
    "\n",
    "# Try importing again after installation\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    print(\"Successfully imported SMOTE after installation\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing SMOTE after installation: {e}\")\n",
    "\n",
    "# Check scikit-learn version compatibility\n",
    "print(f\"Required scikit-learn version for imbalanced-learn: 1.0.2 or later\")\n",
    "print(f\"Current scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# If scikit-learn version is incompatible, update it\n",
    "if sklearn.__version__ < '1.0.2':\n",
    "    print(\"Updating scikit-learn...\")\n",
    "    %pip install --upgrade scikit-learn\n",
    "    import sklearn\n",
    "    print(f\"Updated scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# Try importing SMOTE one more time\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    print(\"Successfully imported SMOTE after all updates\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing SMOTE after all updates: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sample_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProviding academic advice and support to students throughout their studies\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_label_gb\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_job\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Job Description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_job\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mpredict_label_gb\u001b[0;34m(job_description)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_label_gb\u001b[39m(job_description):\n\u001b[0;32m----> 2\u001b[0m     cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mclean_text\u001b[49m(job_description)\n\u001b[1;32m      3\u001b[0m     tfidf_vector \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform([cleaned])\n\u001b[1;32m      4\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m best_models[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradientBoosting\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(tfidf_vector)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "def predict_label_gb(job_description):\n",
    "    cleaned = clean_text(job_description)\n",
    "    tfidf_vector = vectorizer.transform([cleaned])\n",
    "    prediction = best_models['GradientBoosting'].predict(tfidf_vector)\n",
    "    return le.inverse_transform(prediction)[0]\n",
    "\n",
    "# Example usage\n",
    "sample_job = \"Providing academic advice and support to students throughout their studies\"\n",
    "predicted_label = predict_label_gb(sample_job)\n",
    "print(f\"Sample Job Description: {sample_job}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“virtual”env”",
   "language": "python",
   "name": "myvirtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
