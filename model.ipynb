{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression best params: {'C': 10}\n",
      "LogisticRegression best score: 0.8943961352657004\n",
      "SVC best params: {'C': 10, 'kernel': 'linear'}\n",
      "SVC best score: 0.9121739130434783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest best params: {'n_estimators': 50}\n",
      "RandomForest best score: 0.8722705314009662\n",
      "LogisticRegression classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response B       0.73      0.89      0.80        18\n",
      "  Response C       0.93      0.87      0.90        30\n",
      "  Response D       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.86        57\n",
      "   macro avg       0.89      0.84      0.86        57\n",
      "weighted avg       0.88      0.86      0.86        57\n",
      "\n",
      "SVC classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response B       0.73      0.89      0.80        18\n",
      "  Response C       0.92      0.80      0.86        30\n",
      "  Response D       0.78      0.78      0.78         9\n",
      "\n",
      "    accuracy                           0.82        57\n",
      "   macro avg       0.81      0.82      0.81        57\n",
      "weighted avg       0.84      0.82      0.83        57\n",
      "\n",
      "RandomForest classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response B       0.80      0.89      0.84        18\n",
      "  Response C       0.93      0.93      0.93        30\n",
      "  Response D       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.89        57\n",
      "   macro avg       0.91      0.87      0.88        57\n",
      "weighted avg       0.90      0.89      0.90        57\n",
      "\n",
      "Best model saved as best_model_random_forest.pkl\n",
      "Job Description: To work collaboratively with the Director, Pro Vice-Chancellor (Enterprise and External Engagement) and Directors of Academic Partnerships, and Global Engagement, on the development and quality management of UK and Transnational Education (TNE) collaborative partners.\n",
      "True Label: Response B\n",
      "Predicted Label: Response C\n",
      "---\n",
      "Job Description: To work collaboratively with the Director, Pro Vice-Chancellor (Enterprise and External Engagement) and Directors of Academic Partnerships, and Global Engagement, on the development and quality management of UK and Transnational Education (TNE) collaborative partners.\n",
      "True Label: Response B\n",
      "Predicted Label: Response C\n",
      "---\n",
      "Job Description: To assume responsibility for specific projects and areas of operation as identified by the Deputy Director of Finance, to include but not limited to:\n",
      "True Label: Response D\n",
      "Predicted Label: Response B\n",
      "---\n",
      "Job Description: Develop and maintain networks, both within the University and outside, to ensure the application of the most up-to-date and effective practice.\n",
      "True Label: Response C\n",
      "Predicted Label: Response B\n",
      "---\n",
      "Job Description: Develop and maintain networks, both within the University and outside, to ensure the application of the most up-to-date and effective practice.\n",
      "True Label: Response C\n",
      "Predicted Label: Response B\n",
      "---\n",
      "Job Description: To assume responsibility for specific projects and areas of operation as identified by the Deputy Director of Finance, to include but not limited to:\n",
      "True Label: Response D\n",
      "Predicted Label: Response B\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('data.csv', encoding='UTF-8-SIG')\n",
    "\n",
    "# Preprocess the data\n",
    "X = df['Code.1']\n",
    "y = df['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Define the models and parameters for GridSearchCV\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'SVC': SVC(),\n",
    "    'RandomForest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'LogisticRegression': {'C': [0.1, 1, 10]},\n",
    "    'SVC': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'RandomForest': {'n_estimators': [50, 100, 200]}\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "for model_name in models:\n",
    "    grid = GridSearchCV(models[model_name], params[model_name], cv=5, scoring='accuracy')\n",
    "    grid.fit(X_train_tfidf, y_train)\n",
    "    best_models[model_name] = grid.best_estimator_\n",
    "    print(model_name + ' best params: ' + str(grid.best_params_))\n",
    "    print(model_name + ' best score: ' + str(grid.best_score_))\n",
    "\n",
    "# Evaluate the best models on the test set\n",
    "for model_name in best_models:\n",
    "    y_pred = best_models[model_name].predict(X_test_tfidf)\n",
    "    print(model_name + ' classification report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model (Random Forest)\n",
    "best_model = best_models['RandomForest']\n",
    "joblib.dump(best_model, 'best_model_random_forest.pkl')\n",
    "print('Best model saved as best_model_random_forest.pkl')\n",
    "\n",
    "# Analyze misclassifications\n",
    "misclassified = X_test[y_test != y_pred]\n",
    "misclassified_labels = y_test[y_test != y_pred]\n",
    "misclassified_preds = y_pred[y_test != y_pred]\n",
    "\n",
    "# Display misclassified examples\n",
    "for i in range(len(misclassified)):\n",
    "    print('Job Description:', misclassified.iloc[i])\n",
    "    print('True Label:', misclassified_labels.iloc[i])\n",
    "    print('Predicted Label:', misclassified_preds[i])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from imbalanced-learn) (3.2.0)\n",
      "Downloading imbalanced_learn-0.12.3-py3-none-any.whl (258 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /Users/leandrenash/anaconda3/envs/myvirutalenv/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl (278 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2024.5.15 tqdm-4.66.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Response C    136\n",
      "Response B     96\n",
      "Response D     50\n",
      "Name: count, dtype: int64\n",
      "LogisticRegression best params: {'C': 10}\n",
      "LogisticRegression best score: 0.9633100233100234\n",
      "SVC best params: {'C': 1, 'kernel': 'rbf'}\n",
      "SVC best score: 0.9603263403263403\n",
      "RandomForest best params: {'n_estimators': 100}\n",
      "RandomForest best score: 0.9632634032634033\n",
      "LogisticRegression classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response B       1.00      1.00      1.00        27\n",
      "  Response C       1.00      1.00      1.00        29\n",
      "  Response D       1.00      1.00      1.00        26\n",
      "\n",
      "    accuracy                           1.00        82\n",
      "   macro avg       1.00      1.00      1.00        82\n",
      "weighted avg       1.00      1.00      1.00        82\n",
      "\n",
      "SVC classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response B       1.00      1.00      1.00        27\n",
      "  Response C       1.00      1.00      1.00        29\n",
      "  Response D       1.00      1.00      1.00        26\n",
      "\n",
      "    accuracy                           1.00        82\n",
      "   macro avg       1.00      1.00      1.00        82\n",
      "weighted avg       1.00      1.00      1.00        82\n",
      "\n",
      "RandomForest classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Response B       0.93      1.00      0.96        27\n",
      "  Response C       1.00      0.93      0.96        29\n",
      "  Response D       1.00      1.00      1.00        26\n",
      "\n",
      "    accuracy                           0.98        82\n",
      "   macro avg       0.98      0.98      0.98        82\n",
      "weighted avg       0.98      0.98      0.98        82\n",
      "\n",
      "Best model saved as best_model_random_forest.pkl\n",
      "Job Description:   (0, 39)\t0.1429497041646604\n",
      "  (0, 73)\t0.15318445904748504\n",
      "  (0, 74)\t0.15318445904748504\n",
      "  (0, 154)\t0.15318445904748504\n",
      "  (0, 160)\t0.15318445904748504\n",
      "  (0, 165)\t0.07915922468578117\n",
      "  (0, 186)\t0.5061679962231661\n",
      "  (0, 190)\t0.45030650071311007\n",
      "  (0, 192)\t0.5061679962231661\n",
      "  (0, 271)\t0.10188786442272936\n",
      "  (0, 273)\t0.15318445904748504\n",
      "  (0, 673)\t0.07611490317966772\n",
      "  (0, 685)\t0.11976705855498869\n",
      "  (0, 739)\t0.13530526491522568\n",
      "  (0, 745)\t0.13530526491522568\n",
      "  (0, 753)\t0.15318445904748504\n",
      "  (0, 755)\t0.11976705855498869\n",
      "  (0, 761)\t0.12411987551514847\n",
      "  (0, 823)\t0.1429497041646604\n",
      "True Label: Response C\n",
      "Predicted Label: Response B\n",
      "---\n",
      "Job Description:   (0, 39)\t0.1429497041646604\n",
      "  (0, 73)\t0.15318445904748504\n",
      "  (0, 74)\t0.15318445904748504\n",
      "  (0, 154)\t0.15318445904748504\n",
      "  (0, 160)\t0.15318445904748504\n",
      "  (0, 165)\t0.07915922468578117\n",
      "  (0, 186)\t0.5061679962231661\n",
      "  (0, 190)\t0.45030650071311007\n",
      "  (0, 192)\t0.5061679962231661\n",
      "  (0, 271)\t0.10188786442272936\n",
      "  (0, 273)\t0.15318445904748504\n",
      "  (0, 673)\t0.07611490317966772\n",
      "  (0, 685)\t0.11976705855498869\n",
      "  (0, 739)\t0.13530526491522568\n",
      "  (0, 745)\t0.13530526491522568\n",
      "  (0, 753)\t0.15318445904748504\n",
      "  (0, 755)\t0.11976705855498869\n",
      "  (0, 761)\t0.12411987551514847\n",
      "  (0, 823)\t0.1429497041646604\n",
      "True Label: Response C\n",
      "Predicted Label: Response B\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import joblib\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('data.csv', encoding='UTF-8-SIG')\n",
    "\n",
    "# Preprocess the data\n",
    "X = df['Code.1']\n",
    "y = df['Label']\n",
    "\n",
    "# Remove the class with very few samples\n",
    "df = df[df['Label'] != 'Response A']\n",
    "X = df['Code.1']\n",
    "y = df['Label']\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply text preprocessing\n",
    "X = X.apply(preprocess_text)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Check class distribution\n",
    "print(y.value_counts())\n",
    "\n",
    "# Use SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "X_tfidf_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the models and parameters for GridSearchCV\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'SVC': SVC(probability=True),\n",
    "    'RandomForest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'LogisticRegression': {'C': [0.1, 1, 10]},\n",
    "    'SVC': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'RandomForest': {'n_estimators': [50, 100, 200]}\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "for model_name in models:\n",
    "    grid = GridSearchCV(models[model_name], params[model_name], cv=5, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid.best_estimator_\n",
    "    print(model_name + ' best params: ' + str(grid.best_params_))\n",
    "    print(model_name + ' best score: ' + str(grid.best_score_))\n",
    "\n",
    "# Evaluate the best models on the test set\n",
    "for model_name in best_models:\n",
    "    y_pred = best_models[model_name].predict(X_test)\n",
    "    print(model_name + ' classification report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the best model (Random Forest)\n",
    "best_model = best_models['RandomForest']\n",
    "joblib.dump(best_model, 'best_model_random_forest.pkl')\n",
    "print('Best model saved as best_model_random_forest.pkl')\n",
    "\n",
    "# Analyze misclassifications\n",
    "misclassified_indices = np.where(y_test != y_pred)[0]\n",
    "misclassified = X_test[misclassified_indices]\n",
    "misclassified_labels = y_test.iloc[misclassified_indices]\n",
    "misclassified_preds = y_pred[misclassified_indices]\n",
    "\n",
    "# Display misclassified examples\n",
    "for i in range(misclassified.shape[0]):\n",
    "    print('Job Description:', misclassified[i])\n",
    "    print('True Label:', misclassified_labels.iloc[i])\n",
    "    print('Predicted Label:', misclassified_preds[i])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for the new job description:\n",
      "[[0.05 0.87 0.08]]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "best_model = joblib.load('best_model_random_forest.pkl')\n",
    "\n",
    "# Function to preprocess and predict a new job description\n",
    "def predict_new_description(description, model, vectorizer):\n",
    "    description_tfidf = vectorizer.transform([description])\n",
    "    probabilities = model.predict_proba(description_tfidf)\n",
    "    return probabilities\n",
    "\n",
    "# Example new job description\n",
    "new_description = \"To manage, train and mentor the Accommodation and Conference Assistant and any temporary conference assistants\"\n",
    "\n",
    "# Predict probabilities for the new job description\n",
    "probabilities = predict_new_description(new_description, best_model, vectorizer)\n",
    "print('Probabilities for the new job description:')\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After fitting the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Provide all forms of Joinery duties and tasks in which you are competent within the University Estate possessing at least five years trade experience. Working with the team across various other construction trades.\n",
      "Predicted Label: Response C\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the best model (Random Forest) and the vectorizer\n",
    "best_model = joblib.load('best_model_random_forest.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Job description to test\n",
    "test_description = \"Provide all forms of Joinery duties and tasks in which you are competent within the University Estate possessing at least five years trade experience. Working with the team across various other construction trades.\"\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the job description\n",
    "test_description_preprocessed = preprocess_text(test_description)\n",
    "\n",
    "# Vectorize the preprocessed job description using the loaded TF-IDF vectorizer\n",
    "test_description_tfidf = vectorizer.transform([test_description_preprocessed])\n",
    "\n",
    "# Predict the label for the job description using the best model (Random Forest)\n",
    "predicted_label = best_model.predict(test_description_tfidf)\n",
    "\n",
    "# Display the job description and its predicted label\n",
    "print('Job Description:', test_description)\n",
    "print('Predicted Label:', predicted_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Provide support and guidance to Contractors engaged in Fire door campus works and act as a focal point ensuring a fully compliant Fire door install is delivered to the Estate.\n",
      "Predicted Label: Response C\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the best model (Random Forest) and the vectorizer\n",
    "best_model = joblib.load('best_model_random_forest.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Job description to test\n",
    "test_description = \"Provide support and guidance to Contractors engaged in Fire door campus works and act as a focal point ensuring a fully compliant Fire door install is delivered to the Estate.\"\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the job description\n",
    "test_description_preprocessed = preprocess_text(test_description)\n",
    "\n",
    "# Vectorize the preprocessed job description using the loaded TF-IDF vectorizer\n",
    "test_description_tfidf = vectorizer.transform([test_description_preprocessed])\n",
    "\n",
    "# Predict the label for the job description using the best model (Random Forest)\n",
    "predicted_label = best_model.predict(test_description_tfidf)\n",
    "\n",
    "# Display the job description and its predicted label\n",
    "print('Job Description:', test_description)\n",
    "print('Predicted Label:', predicted_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: To continually review and reflect upon the currency and operation of the University’s Academic Regulations, benchmarking against the sector and regulatory requirements as appropriate, and when required to drive forward any changes necessary to the Academic Regulations.\n",
      "Predicted Label: Response B\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the best model (Random Forest) and the vectorizer\n",
    "best_model = joblib.load('best_model_random_forest.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Job description to test\n",
    "test_description = \"To continually review and reflect upon the currency and operation of the University’s Academic Regulations, benchmarking against the sector and regulatory requirements as appropriate, and when required to drive forward any changes necessary to the Academic Regulations.\"\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the job description\n",
    "test_description_preprocessed = preprocess_text(test_description)\n",
    "\n",
    "# Vectorize the preprocessed job description using the loaded TF-IDF vectorizer\n",
    "test_description_tfidf = vectorizer.transform([test_description_preprocessed])\n",
    "\n",
    "# Predict the label for the job description using the best model (Random Forest)\n",
    "predicted_label = best_model.predict(test_description_tfidf)\n",
    "\n",
    "# Display the job description and its predicted label\n",
    "print('Job Description:', test_description)\n",
    "print('Predicted Label:', predicted_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: To continually review and reflect upon the currency and operation of the University’s Academic Regulations, benchmarking against the sector and regulatory requirements as appropriate, and when required to drive forward any changes necessary to the Academic Regulations.\n",
      "Predicted Label: Response B\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the best model (Random Forest) and the vectorizer\n",
    "best_model = joblib.load('best_model_random_forest.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Job description to test\n",
    "test_description = \"To continually review and reflect upon the currency and operation of the University’s Academic Regulations, benchmarking against the sector and regulatory requirements as appropriate, and when required to drive forward any changes necessary to the Academic Regulations.\"\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the job description\n",
    "test_description_preprocessed = preprocess_text(test_description)\n",
    "\n",
    "# Vectorize the preprocessed job description using the loaded TF-IDF vectorizer\n",
    "test_description_tfidf = vectorizer.transform([test_description_preprocessed])\n",
    "\n",
    "# Predict the label for the job description using the best model (Random Forest)\n",
    "predicted_label = best_model.predict(test_description_tfidf)\n",
    "\n",
    "# Display the job description and its predicted label\n",
    "print('Job Description:', test_description)\n",
    "print('Predicted Label:', predicted_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Establish a training model for the University for monitoring and evaluation of widening participation initiatives relating to the APP.\n",
      "Predicted Label: Response B\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the best model (Random Forest) and the vectorizer\n",
    "best_model = joblib.load('best_model_random_forest.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Job description to test\n",
    "test_description = \"Establish a training model for the University for monitoring and evaluation of widening participation initiatives relating to the APP.\"\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the job description\n",
    "test_description_preprocessed = preprocess_text(test_description)\n",
    "\n",
    "# Vectorize the preprocessed job description using the loaded TF-IDF vectorizer\n",
    "test_description_tfidf = vectorizer.transform([test_description_preprocessed])\n",
    "\n",
    "# Predict the label for the job description using the best model (Random Forest)\n",
    "predicted_label = best_model.predict(test_description_tfidf)\n",
    "\n",
    "# Display the job description and its predicted label\n",
    "print('Job Description:', test_description)\n",
    "print('Predicted Label:', predicted_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description: Provide support and guidance to Contractors engaged in Fire door campus works and act as a focal point ensuring a fully compliant Fire door install is delivered to the Estate.\n",
      "Predicted Label: Response C\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the best model (Random Forest) and the vectorizer\n",
    "best_model = joblib.load('best_model_random_forest.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Job description to test\n",
    "test_description = \"Provide support and guidance to Contractors engaged in Fire door campus works and act as a focal point ensuring a fully compliant Fire door install is delivered to the Estate.\"\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the job description\n",
    "test_description_preprocessed = preprocess_text(test_description)\n",
    "\n",
    "# Vectorize the preprocessed job description using the loaded TF-IDF vectorizer\n",
    "test_description_tfidf = vectorizer.transform([test_description_preprocessed])\n",
    "\n",
    "# Predict the label for the job description using the best model (Random Forest)\n",
    "predicted_label = best_model.predict(test_description_tfidf)\n",
    "\n",
    "# Display the job description and its predicted label\n",
    "print('Job Description:', test_description)\n",
    "print('Predicted Label:', predicted_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the best model (Random Forest) and the vectorizer\n",
    "best_model = joblib.load('best_model_random_forest.pkl')\n",
    "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "\n",
    "# Job description to test\n",
    "test_description = \"Establish a training model for the University for monitoring and evaluation of widening participation initiatives relating to the APP.\"\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the job description\n",
    "test_description_preprocessed = preprocess_text(test_description)\n",
    "\n",
    "# Vectorize the preprocessed job description using the loaded TF-IDF vectorizer\n",
    "test_description_tfidf = vectorizer.transform([test_description_preprocessed])\n",
    "\n",
    "# Predict the label for the job description using the best model (Random Forest)\n",
    "predicted_label = best_model.predict(test_description_tfidf)\n",
    "\n",
    "# Display the job description and its predicted label\n",
    "print('Job Description:', test_description)\n",
    "print('Predicted Label:', predicted_label[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“virtual”env”",
   "language": "python",
   "name": "myvirtualenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
